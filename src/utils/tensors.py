# --------------------------------------------------------
# References:
# I-JEPA: https://github.com/facebookresearch/ijepa
# --------------------------------------------------------
#

import math

import torch

from logging import getLogger

logger = getLogger()


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    """
    截断正态分布初始化
    
    参数:
        tensor: 要初始化的张量
        mean: 均值
        std: 标准差
        a: 最小截断值
        b: 最大截断值
    
    返回:
        初始化后的张量
    """
    # 使用PyTorch内置的trunc_normal_或实现自定义逻辑
    if hasattr(torch.nn.init, 'trunc_normal_'):
        return torch.nn.init.trunc_normal_(tensor, mean=mean, std=std, a=a, b=b)
    else:
        # 手动实现截断正态分布初始化
        return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def apply_masks(x, masks):
    """
    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]
    :param masks: list of tensors containing indices of patches in [N] to keep
    """
    all_x = []
    for m in masks:
        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))
        all_x += [torch.gather(x, dim=1, index=mask_keep)]
    return torch.cat(all_x, dim=0)


def repeat_interleave_batch(tensor, batch_size):
    """
    在批次维度上重复张量
    
    参数:
        tensor: 输入张量
        batch_size: 目标批次大小
    
    返回:
        重复后的张量
    """
    # 对于一维张量
    if tensor.dim() == 1:
        return tensor.repeat(batch_size)
    # 对于多维张量，只在第一个维度（批次维度）重复
    else:
        repeat_shape = [batch_size] + [1] * (tensor.dim() - 1)
        return tensor.repeat(*repeat_shape)
